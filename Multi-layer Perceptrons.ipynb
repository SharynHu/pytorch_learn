{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9165f852",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec94e8",
   "metadata": {},
   "source": [
    "## MLP Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b02eb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\\begin{aligned}\n",
    "    \\mathbf{H} & = \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}), \\\\\n",
    "    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\\\\\n",
    "\\end{aligned}\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21a35a",
   "metadata": {},
   "source": [
    "### MLPs are universal approximators.\n",
    "For certain choices of the activation function, it is widely known that MLPs are universal approximators. Even with a single-hidden-layer network, given enough nodes (possibly absurdly many), and the right set of weights, we can model any function, though actually learning that function is the hard part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4dae7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
