{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn` is the neural network API in pytorch, and `torch.nn.Module` is the base class for all neural network mudules in pytorch.\n",
    "\n",
    "Here is the inheritance structure for the `torch.nn.Module` class:\n",
    "![image](./noteimg/torch_nn_Module_inheritance_structure.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.Module`  Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `nn.Module.__init__`\n",
    "`nn.Module.__init__` basically does:\n",
    "1. call `torch._C._log_api_usage_once(\"python.nn_module\")` to **monitor and rocord the usage of APIs**.\n",
    "2. **initialize important member variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an detailed list of parameters initialized:\n",
    "```python\n",
    "self.training=True               # control whether it is training/testing\n",
    "self._parameters = OrderedDict() # save parameters chaning with BP\n",
    "self._buffers = OrderedDict()    # save parameters not changing with BP\n",
    "self._non_persistent_buffers_set = set()\n",
    "self._backward_hooks = OrderedDict() # hooks to be called after BP\n",
    "self._forward_hooks = OrderedDict() # hooks to be called after forward\n",
    "self._forward_prehooks = OrderedDict() # hooks to be called before forward\n",
    "self._state_dict_hooks = OrderedDict() # hooks to be called after getting state_dict\n",
    "self._load_state_dict_pre_hooks = OrderedDict() # hooks to be called before loading state_dict\n",
    "self._modules = OrderedDict()    # sub modules\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- when we are initializing our self-defined module, we need to call `super().__init__` first, or else the above member variables are not created, which will give us error when we call other methods/functions/attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `nn.Module.train` and `nn.Module.eval`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Module` use its member variable `self.training` to determine whether it is traning session or testing session.\n",
    "The method `nn.Module.train` set `self.training` to `mode` for this module and all its sub-modules.\n",
    "```python\n",
    "def train(self: T, mode: bool = True) -> T:\n",
    "    self.training = mode\n",
    "    for module in self.children():\n",
    "        module.train(mode)\n",
    "    return self\n",
    "```\n",
    "\n",
    "`nn.Module.eval` simply calls `self.train(False)`\n",
    "```python\n",
    "def eval(self: T) -> T:\n",
    "        return self.train(False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `nn.Module.requires_grad_`  and `nn.Module.zero_grad_`\n",
    "These two methods are used to modify the status of the gradients of the parameters or simply clear the gradients.\n",
    "```python\n",
    "    def requires_grad_(self: T, requires_grad: bool = True) -> T:\n",
    "        r\"\"\"\n",
    "        This method sets the parameters' :attr:`requires_grad` attributes\n",
    "        in-place.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_(requires_grad)\n",
    "        return self\n",
    "```\n",
    "   \n",
    "```python\n",
    "def zero_grad(self, set_to_none: bool = False) -> None:\n",
    "    if getattr(self, '_is_replica', False):\n",
    "        warnings.warn(\n",
    "            \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\n",
    "            \"The parameters are copied (in a differentiable manner) from the original module. \"\n",
    "            \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n",
    "            \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n",
    "\n",
    "    for p in self.parameters():\n",
    "        if p.grad is not None:\n",
    "            if set_to_none:\n",
    "                p.grad = None\n",
    "            else:\n",
    "                if p.grad.grad_fn is not None:\n",
    "                    p.grad.detach_()\n",
    "                else:\n",
    "                    p.grad.requires_grad_(False)\n",
    "                p.grad.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter Conversion / Transfer\n",
    "We can convert all the **parameters** and **buffers** of a module to another data type or transfer them to another device.\n",
    "`nn.Module` provides 8 such methods, which are:\n",
    "1. `nn.Module.cpu`: transfer all parameters and buffers to CPU.\n",
    "2. `nn.Module.type`: convert all parameters and buffers to a certain type.\n",
    "3. `nn.Module.cuda`: tansfer all parameters and buffers to GPU.\n",
    "4. `nn.Module.float`: convert all **floating point** paramaters and buffers to `float32`.\n",
    "5. `nn.Module.double`: convert all **floating point** parameters and buffers to `double`.\n",
    "6. `nn.Module.half`: convert all **floating point** parameters and buffers to `float16`.\n",
    "7. `nn.Module.bfloat16`: convert all **floating point** parameters and buffers to `bfloat16`.\n",
    "8. `nn.Module.to`: Moves and/or casts the parameters and buffers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 `nn.Module._apply`\n",
    "These functions are implemented by calling `nn.Module._apply(fn)`.\n",
    "```python\n",
    "def _apply(self, fn):\n",
    "    # 对子模块进行递归调用\n",
    "    for module in self.children():\n",
    "        module._apply(fn)\n",
    "\n",
    "    # 为了 BC-breaking 而新增了一个 tensor 类型判断\n",
    "    def compute_should_use_set_data(tensor, tensor_applied):\n",
    "        if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
    "            return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # 处理参数及其gradint\n",
    "    for key, param in self._parameters.items():\n",
    "        if param is not None:\n",
    "            # Tensors stored in modules are graph leaves, and we don't want to\n",
    "            # track autograd history of `param_applied`, so we have to use\n",
    "            # `with torch.no_grad():`\n",
    "            with torch.no_grad():\n",
    "                param_applied = fn(param)\n",
    "            should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
    "            if should_use_set_data:\n",
    "                param.data = param_applied\n",
    "            else:\n",
    "                assert isinstance(param, Parameter)\n",
    "                assert param.is_leaf\n",
    "                self._parameters[key] = Parameter(param_applied, param.requires_grad)\n",
    "            if param.grad is not None:\n",
    "                with torch.no_grad():\n",
    "                    grad_applied = fn(param.grad)\n",
    "                should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)\n",
    "                if should_use_set_data:\n",
    "                    param.grad.data = grad_applied\n",
    "                else:\n",
    "                    assert param.grad.is_leaf\n",
    "                    self._parameters[key].grad = grad_applied.requires_grad_(param.grad.requires_grad)\n",
    "\n",
    "    # 处理 buffers\n",
    "    for key, buf in self._buffers.items():\n",
    "        if buf is not None:\n",
    "            self._buffers[key] = fn(buf)\n",
    "    return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 `nn.Module.apply`\n",
    "`nn.Module.apply` wraps the private method `nn.Module._apply` for public use.\n",
    "\n",
    "It simply calls the `nn.Module._apply` method.\n",
    "\n",
    "The definition is as below:\n",
    "```python\n",
    "def apply(self: T, fn: Callable[['Module'], None]) -> T:\n",
    "    for module in self.children():\n",
    "        module.apply(fn)\n",
    "    fn(self)\n",
    "    return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Methods to modify attributes\n",
    "There are 3 methods to modify the member variables of a Module.\n",
    "1. `nn.Module.add_module`: it add sub modules to `nn.Module._modules`;\n",
    "2. `nn.Module.register_parameter`:it updates `nn.Module._parameters` and the added parameter can be updated via BP.\n",
    "3. `nn.Module.register_buffer`: it updates `nn.Module._buffers`. If the buffer is not persistant, it will be added to `self._non_persistant_buffers_set`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we modify attrutes of a module using the expession `self.attr = val`, and it calls the method `self.__setattr__` that is overwritten by `nn.Module`.\n",
    "The signature of this method is:\n",
    "```python\n",
    "def __setattr__(self, name: str, value: Union[Tensor, 'Module']):\n",
    "    def remove_from(*dicts_or_sets):\n",
    "        for d in dicts_or_sets:\n",
    "            if name in d:\n",
    "                if isinstance(d, dict):\n",
    "                    del d[name]\n",
    "                else:\n",
    "                    d.discard(name)\n",
    "\n",
    "    params = self.__dict__.get('_parameters')\n",
    "    # if the value is a Parameter\n",
    "    if isinstance(value, Parameter):\n",
    "        if params is None:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign parameters before Module.__init__() call\")\n",
    "        remove_from(self.__dict__, self._buffers, self._modules, self._non_persistent_buffers_set)\n",
    "        self.register_parameter(name, value)\n",
    "        elif params is not None and name in params:\n",
    "        if value is not None:\n",
    "            raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n",
    "                            \"(torch.nn.Parameter or None expected)\"\n",
    "                            .format(torch.typename(value), name))\n",
    "        self.register_parameter(name, value)\n",
    "    else:\n",
    "        modules = self.__dict__.get('_modules')\n",
    "        if isinstance(value, Module):\n",
    "            if modules is None:\n",
    "                raise AttributeError(\n",
    "                    \"cannot assign module before Module.__init__() call\")\n",
    "            remove_from(self.__dict__, self._parameters, self._buffers, self._non_persistent_buffers_set)\n",
    "            modules[name] = value\n",
    "        elif modules is not None and name in modules:\n",
    "            if value is not None:\n",
    "                raise TypeError(\"cannot assign '{}' as child module '{}' \"\n",
    "                                \"(torch.nn.Module or None expected)\"\n",
    "                                .format(torch.typename(value), name))\n",
    "            modules[name] = value\n",
    "        else:\n",
    "            buffers = self.__dict__.get('_buffers')\n",
    "            if buffers is not None and name in buffers:\n",
    "                if value is not None and not isinstance(value, torch.Tensor):\n",
    "                    raise TypeError(\"cannot assign '{}' as buffer '{}' \"\n",
    "                                    \"(torch.Tensor or None expected)\"\n",
    "                                    .format(torch.typename(value), name))\n",
    "                buffers[name] = value\n",
    "            else:\n",
    "                object.__setattr__(self, name, value)\n",
    "```\n",
    "It accepts only `torch.Tensor` or `torch.nn.Module` as input of the attribute value. And it will do the following things:\n",
    "1. check if this module is initialized properly;\n",
    "2. if the value is a `Parameter`, it will:\n",
    "    1. check if this module is initialized properly.\n",
    "    2. remove `attr` from `self.__dict__, self._buffers, self._modules, self._non_persistent_buffers_set`;\n",
    "    3. register this parameter by calling `self.register_parameter`.\n",
    "3. if the value if not a `Parameter`, which means it's a `Module`:\n",
    "    - check if  `name` is in `params`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
